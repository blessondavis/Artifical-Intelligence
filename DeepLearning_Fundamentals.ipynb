{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_Fundamentals.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dense layer from scratch\n",
        "**Design of a perceptron:** Take the inputs and do a weighted sum of these inputs. Pass it through a non linear activation function(can be sigmoid, tanh, or Relu, or LeakyRelu). \n A perceptron is the fundamental building block of Neural Network. \n",
        "\n",
        "**Activation Functions**: It brings non-linearity to the model making it possible to create complex decision boundary in the feature space.\n"
      ],
      "metadata": {
        "id": "5A0yAH-PAoGv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryx5AV87xrEA"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(DenseLayer, self).__init__()\n",
        "\n",
        "    #Initialize weights and bias\n",
        "    self.W = self.add_weight((input_dim,output_dim))\n",
        "    self.b = self.add_weight((1, output_dim))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Forward propogate the inputs\n",
        "    z = tf.matmul(inputs, self.W)+self.b\n",
        "\n",
        "    # Feed through a non linear activation \n",
        "    output = tf.math.sigmoid(z)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the above function\n",
        "Stacking perceptrons forms neural networks. "
      ],
      "metadata": {
        "id": "uvnKmaDMA9j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "layer = tf.keras.layers.Dense(units=2)"
      ],
      "metadata": {
        "id": "SIyJX16SBSo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantifying Loss\n",
        "Loss measures the cost incurred from incorrect predictions. \n",
        "* MSE is used if we are predicting continous values.\n",
        "* Cross entropy is used incase the output is catagorical."
      ],
      "metadata": {
        "id": "WtAeYeRYDamM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,predicted))"
      ],
      "metadata": {
        "id": "HLDCTxe7DmIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Optimization\n",
        "Once cost function is defined, learning becomes an optimization problem-find weights that achieve the lowest loss.\n",
        "\n",
        "### Gradient Descent Algorithm\n",
        "1. Initialize weights randomly\n",
        "2. Loop until convergence\n",
        "3. Compute gradient --> dJ(W)/dW\n",
        "4. Update weights: W <-- (W-n(dJ(W)/dW))\n",
        "5. Return weights"
      ],
      "metadata": {
        "id": "yi05TlNUE_YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = tf.Variable((tf.random.normal()))\n",
        "\n",
        "while True: \n",
        "\n",
        "  #infinite loop\n",
        "  with tf.GradientTape() as g:\n",
        "    loss = compute_loss(weights)\n",
        "    gradient = g.gradient(loss,weights)\n",
        "\n",
        "  weights = weights - lr*gradient\n",
        "  #lr-->learning rate/step size"
      ],
      "metadata": {
        "id": "t0irnz4PGLE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent Algorithms\n",
        "Taking the right step size matters and determines how well the model converges.\n",
        "1. SGD\n",
        "2. Adam\n",
        "3. Adadelta\n",
        "4. Adagrad\n",
        "5. RMSProp\n",
        "\n",
        "### Putting it all together"
      ],
      "metadata": {
        "id": "LnDYCeDaI8tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([])\n",
        "\n",
        "#choose an optimizer\n",
        "optimizer = tf.keras.optimizer.SGD()\n",
        "\n",
        "while True:\n",
        "  #forward pass\n",
        "  prediction = model(x)\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    #compute loss\n",
        "    loss = compute_loss(y,prediction)\n",
        "  \n",
        "  #update the weights using gradients\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradient(zip(grads, model.trainable_variable)) \n"
      ],
      "metadata": {
        "id": "GbqZSlNQJb4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting and underfitting\n",
        "Machine learning models and especially deep learning models are prone to overfiting: model fails to explain the general trend of the data.\n",
        "\n",
        "### Regularization\n",
        "Technique that constrains our optimization problem to discourage complex models.\n",
        "\n",
        "Need for regularization: Improve generalization of model on unseen data. The goal of the model is to reduce the cost function(computed using training set), but the ultimate goal is to be able to perform well in the test set.\n",
        "\n",
        "**Regularization I**: Dropout\n",
        "\n",
        "During training, we randomly drop some and set some activations in the hidden layer and make them zero. \n",
        "\n",
        "* Typically drop 50% of activations in layer in different iterations.\n",
        "* Forces the network to not rely on any 1 node\n",
        "\n",
        "**Regularization II**: Early Stopping\n",
        "\n",
        "Stop training before we have a chance to overfit.\n",
        "\n",
        "* Split the training set into two parts (test and train). Plot the iteration loss against the loss \n"
      ],
      "metadata": {
        "id": "0zn5_1mJMGvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.Dropout(p = 0.5)"
      ],
      "metadata": {
        "id": "M_t9toPJOSHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
